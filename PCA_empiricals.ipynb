{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression, ElasticNetCV, ElasticNet\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pca_viz import do_PCA\n",
    "from compare import compare_classification_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = './raw/'\n",
    "\n",
    "# Musk dataset\n",
    "df = pd.read_csv(path + 'musk_ver2/clean2.data', header=None)\n",
    "column_names = ['mol_name', 'conf_name']\n",
    "column_names.extend(list(range(1, 163)))\n",
    "column_names.extend(['oxy_dis', 'oxy_x', 'oxy_y', 'oxy_z', 'class_'])\n",
    "df.columns = column_names\n",
    "y_data_musk = df.class_.astype('int64')\n",
    "x_data_musk = df.drop(['class_', 'mol_name', 'conf_name'], axis=1)\n",
    "\n",
    "# colposcopy dataset\n",
    "df_green = pd.read_csv(path + 'colposcopy/green.csv')\n",
    "df_hinselmann = pd.read_csv(path + 'colposcopy/hinselmann.csv')\n",
    "df_schiller = pd.read_csv(path + 'colposcopy/schiller.csv')\n",
    "df = df_green.append([df_hinselmann, df_schiller])\n",
    "df = df.reset_index()\n",
    "del df['index']\n",
    "y_data_colposcopy = df.consensus\n",
    "# columns 62 to 68, starting with \"experts\", are also target labels.\n",
    "# the column 'consensus' is made from these columns\n",
    "x_data_colposcopy = df.iloc[:,:62]\n",
    "\n",
    "# Z-Alizadeh Sani CAD diagnosis dataset\n",
    "df = pd.read_excel(path + 'CAD_diagnosis/CAD_diagnosis.xlsx')\n",
    "y_data_cad = df.Cath.apply(lambda x: 1 if x == 'Cad' else 0)\n",
    "x_data_cad = pd.get_dummies(df.drop('Cath', axis=1), drop_first=True, \n",
    "                        dtype='int64')\n",
    "\n",
    "# Spambase dataset\n",
    "df = pd.read_csv(path + 'spambase/spambase.data', header=None)\n",
    "df.head()\n",
    "y_data_spam = df[57]\n",
    "x_data_spam = df.drop(57, axis=1)\n",
    "\n",
    "# sports articles for objectivity analysis dataset\n",
    "df = pd.read_csv(path + 'sports_articles_objectivity/features.csv')\n",
    "df = df.drop(['TextID', 'URL'], axis=1)\n",
    "y_data_sports = df.Label.apply(lambda x: 1 if x == 'subjective' else 0)\n",
    "x_data_sports = df.drop('Label', axis=1)\n",
    "\n",
    "# sonar detection. mines vs rocks dataset\n",
    "df = pd.read_csv(path + 'sonar_mines_rocks/sonar.all-data', header=None)\n",
    "df.head()\n",
    "y_data_sonar = df[60].apply(lambda x: 1 if x == 'R' else 0)\n",
    "x_data_sonar = df.iloc[:,:60]\n",
    "\n",
    "# first-order theorem proving dataset\n",
    "df = pd.read_csv(path + 'first_order_theorem_proving/train.csv', header=None)\n",
    "df = df.append(pd.read_csv(\n",
    "    path + 'first_order_theorem_proving/test.csv', header=None\n",
    "))\n",
    "df = df.append(pd.read_csv(\n",
    "    path + 'first_order_theorem_proving/validation.csv', header=None\n",
    "))\n",
    "y_data_thm = df[56].apply(lambda x: 1 if x == 1 else 0)\n",
    "x_data_thm = df.iloc[:,:51]\n",
    "\n",
    "# secom dataset\n",
    "y_data = pd.read_csv(\n",
    "    path + 'secom/secom_labels.data', delimiter=' ', header=None\n",
    ")\n",
    "x_data = pd.read_csv(path + 'secom/secom.data', delimiter=' ',header=None)\n",
    "y_data_scm = y_data[0].apply(lambda x: 1 if x == 1 else 0)\n",
    "x_data_scm = x_data.fillna(x_data.mean())\n",
    "\n",
    "# Epileptic seizure recognition dataset \n",
    "df = pd.read_csv(path + 'epileptic_seizure/data.csv')\n",
    "y_data_epi = df['y'].apply(lambda x: 1 if x == 1 else 0)\n",
    "x_data_epi = df.drop(['y', 'Unnamed: 0'], axis=1)\n",
    "\n",
    "# Santander customer satisfaction dataset\n",
    "df = pd.read_csv(path + 'santander_customer_satisfaction/train.csv')\n",
    "y_data_san = df.TARGET\n",
    "x_data_san = df.drop(['TARGET', 'ID'], axis=1)\n",
    "\n",
    "y_datas = [\n",
    "    y_data_musk, y_data_colposcopy, y_data_cad, y_data_spam, y_data_sports,\n",
    "    y_data_sonar, y_data_thm, y_data_scm, y_data_epi, y_data_san\n",
    "]\n",
    "x_datas_original = [\n",
    "    x_data_musk, x_data_colposcopy, x_data_cad, x_data_spam, x_data_sports,\n",
    "    x_data_sonar, x_data_thm, x_data_scm, x_data_epi, x_data_san\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standardize all the features\n",
    "x_datas_std = []\n",
    "for x_data in x_datas_original:\n",
    "    x_datas_std.append(pd.DataFrame(StandardScaler().fit_transform(x_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PCs: 10\n",
      "Total explained variance: 0.7336116567646198\n",
      "PCA completed\n",
      "Number of PCs: 10\n",
      "Total explained variance: 0.8233506321285604\n",
      "PCA completed\n",
      "Number of PCs: 10\n",
      "Total explained variance: 0.427786382709891\n",
      "PCA completed\n",
      "Number of PCs: 10\n",
      "Total explained variance: 0.3806001048512228\n",
      "PCA completed\n",
      "Number of PCs: 10\n",
      "Total explained variance: 0.7457095255196374\n",
      "PCA completed\n",
      "Number of PCs: 10\n",
      "Total explained variance: 0.739275479954541\n",
      "PCA completed\n",
      "Number of PCs: 10\n",
      "Total explained variance: 0.7876884236878547\n",
      "PCA completed\n",
      "Number of PCs: 10\n",
      "Total explained variance: 0.2604670756508573\n",
      "PCA completed\n",
      "Number of PCs: 10\n",
      "Total explained variance: 0.4421220783698221\n",
      "PCA completed\n",
      "Number of PCs: 10\n",
      "Total explained variance: 0.37902430591332814\n",
      "PCA completed\n"
     ]
    }
   ],
   "source": [
    "# generate groups of features for each dataset. \n",
    "# The first two groups come from PCA, with the first being the top 5 principle\n",
    "# components (PCs) and the second the 6th to 10th PCs in terms of explained\n",
    "# variance.\n",
    "# The last 5 are groups of 5 randomly drawn columns from the dataset, without\n",
    "# replacement.\n",
    "x_datas_features_groups = []\n",
    "for x_data in x_datas_std:\n",
    "    # get the PCs\n",
    "    _, pc_data = do_PCA(10, x_data, random_state=RANDOM_STATE)\n",
    "    pca_top_5 = pc_data.iloc[:, :5]\n",
    "    pca_next_5 = pc_data.iloc[:, 5:10]\n",
    "\n",
    "    # get random groups of 5 columns\n",
    "    column_indices = np.array(range(len(x_data.columns)))\n",
    "    np.random.shuffle(column_indices)\n",
    "    rand_features_1 = x_data[column_indices[:5]]\n",
    "    rand_features_2 = x_data[column_indices[5:10]]\n",
    "    rand_features_3 = x_data[column_indices[10:15]]\n",
    "    rand_features_4 = x_data[column_indices[15:20]]\n",
    "    rand_features_5 = x_data[column_indices[20:25]]\n",
    "    \n",
    "    # This will be the various groups of features of one dataset (each group \n",
    "    # to be used in a model instance)\n",
    "    feature_groups_list = [\n",
    "        pca_top_5, pca_next_5, rand_features_1, rand_features_2, \n",
    "        rand_features_3, rand_features_4, rand_features_5\n",
    "    ]\n",
    "    # This will be the list containing data from all the datasets\n",
    "    x_datas_features_groups.append(feature_groups_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing dataset 1\n",
      "Doing dataset 2\n",
      "Doing dataset 3\n",
      "Doing dataset 4\n",
      "Doing dataset 5\n",
      "Doing dataset 6\n",
      "Doing dataset 7\n",
      "Doing dataset 8\n",
      "Doing dataset 9\n",
      "Doing dataset 10\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "results = []\n",
    "# loop thru all the datasets \n",
    "for i in range(10):\n",
    "    print(f'Doing dataset {i + 1}')\n",
    "    # compare_classification_features() automatically loops thru all feature\n",
    "    # groups \n",
    "    result = compare_classification_features(\n",
    "        x_data_list=x_datas_features_groups[i],\n",
    "        y_data=y_datas[i],\n",
    "        num_folds=10,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=False\n",
    "    )\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_r = pd.DataFrame(results)\n",
    "df_r.index.name = 'dataset'\n",
    "df_r.columns = [\n",
    "    'top_5_PCs', 'next_5_PCs', 'rand_features_1', \n",
    "    'rand_features_2', 'rand_features_3', \n",
    "    'rand_features_4', 'rand_features_5'\n",
    "]\n",
    "df_r['rand_features_mean'] = df_r.iloc[:, 2:].mean(axis=1)\n",
    "df_r = df_r[['top_5_PCs', 'next_5_PCs', 'rand_features_mean']]\n",
    "df_r['top_5_wins'] = df_r.top_5_PCs > df_r.next_5_PCs\n",
    "df_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_scm.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "_, pc_data = do_PCA(10, x_data_scm, random_state=RANDOM_STATE)\n",
    "pca_top_5 = pc_data.iloc[:, :5]\n",
    "pca_next_5 = pc_data.iloc[:, 5:10]\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100,\n",
    "                             random_state=RANDOM_STATE)\n",
    "lr = LogisticRegression(C=1000, random_state=RANDOM_STATE,\n",
    "                        solver='liblinear', max_iter=500)\n",
    "knn = KNeighborsClassifier(n_neighbors=20, weights='distance')\n",
    "\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('rfc', rfc), ('lr', lr),\n",
    "                ('knn', knn)],\n",
    "    voting='soft'\n",
    ")\n",
    "kf = KFold(n_splits=3, random_state=RANDOM_STATE, shuffle=True)\n",
    "for train_index, test_index in kf.split(x_data_scm):\n",
    "    x_train = x_data_scm.iloc[train_index]\n",
    "    y_train = y_data_scm.iloc[train_index]\n",
    "    x_test = x_data_scm.iloc[test_index]\n",
    "    y_test = y_data_scm.iloc[test_index]\n",
    "\n",
    "    eclf.fit(x_train, y_train)\n",
    "    train_pred = eclf.predict(x_train)\n",
    "    test_pred = eclf.predict(x_test)\n",
    "    train_score = roc_auc_score(y_train, train_pred)\n",
    "    test_score = roc_auc_score(y_test, test_pred)\n",
    "\n",
    "    print(f'train_score: {train_score}')\n",
    "    print(f'test_score: {test_score}')\n",
    "\n",
    "print(train_pred)\n",
    "print(test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
